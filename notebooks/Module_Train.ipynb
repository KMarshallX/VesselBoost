{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **VesselBoost Toolbox**\n",
    "## Train your own models\n",
    "VesselBoost also allows you to train new models based on UNET-3D architecture [Çiçek, Ö., et al. (2016)] used in our framework. \\\n",
    "This is analogous to our boost module, but here you would train models leveraging ground-truth segmentations. \\\n",
    "Like **Boost**, to use this module, first ensure that the raw images and segmentation files are stored separately (e.g., you can store your images in path_to_directory/data/image/ and the corresponding segmentations in path_to_directory/data/label/).\\\n",
    "Additionally, please make sure that the segmentation file name is the same as the raw image file name or that it matches with the following format:\\\n",
    "\n",
    "> **Raw Image**: TOF_3895.nii.gz\\\n",
    "> **Base Segmentation**: seg_TOF_3895.nii.gz or TOF_3895_seg.nii.gz, ensuring that the segmentation file name contains the \"TOF_3895\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the usage of this module, we will download a public MRA dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 164M/164M [00:01<00:00, 105Mbytes/s]\n",
      "100%|█████████████████████████████████████| 327M/327M [00:01<00:00, 169Mbytes/s]\n"
     ]
    }
   ],
   "source": [
    "# Download the data\n",
    "!mkdir -p ./../data/img/\n",
    "!mkdir -p ./../data/seg/\n",
    "!osf -p nr6gc fetch /osfstorage/twoEchoTOF/withSkull/GRE_3D_400um_TR20_FA18_TE7p5_14_sli52_FCY_GMP_BW200_32_biasCor.nii ./../data/img/GRE_400um.nii\n",
    "!osf -p nr6gc fetch /osfstorage/twoEchoTOF/seg/seg_GRE_3D_400um_TR20_FA18_TE7p5_14_sli52_FCY_GMP_BW200_32_biasCor_H75_L55_C10.nii ./../data/seg/seg_GRE_400um.nii"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to *predict* module, there are four distinct settings for data preprocessing:\n",
    " - Set prep_mode to 1 for N4 bias field correction only;\n",
    " - Set prep_mode to 2 for denoising only;\n",
    " - Set prep_mode to 3 for both.;\n",
    " - Set prep_mode to 0 for no preprocessing.\n",
    "\n",
    "If preprocessing is required, please explicitly pass a path to store the preprocessed images to **--ps_path** argument -- for more details, please refer to *Module_Prediction* notebook.\n",
    "Below we illustrate the usage of this module with a sample MRA image and without preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-23 17:16:16,000 - INFO - Training session will start shortly..\n",
      "2026-02-23 17:16:16,000 - INFO - Parameters Info:\n",
      "2026-02-23 17:16:16,000 - INFO - *************************************************************\n",
      "2026-02-23 17:16:16,000 - INFO - Input image path: ./../data/img/, Segmentation path: ./../data/seg/, Prep_mode: 4\n",
      "2026-02-23 17:16:16,000 - INFO - Epoch number: 10, Learning rate: 0.001\n",
      "2026-02-23 17:16:16,000 - INFO - Preprocessing aborted by user\n",
      "2026-02-23 17:16:16,111 - INFO - Trainer initialized on device: cuda\n",
      "2026-02-23 17:16:16,113 - INFO - Matched image: GRE_400um.nii with segmentation: seg_GRE_400um.nii\n",
      "2026-02-23 17:16:16,113 - INFO - Found 1 image pairs\n",
      "2026-02-23 17:16:16,900 - INFO - Initialized loader for GRE_400um.nii\n",
      "2026-02-23 17:16:17,140 - INFO - Training with effective batch size: 6\n",
      "2026-02-23 17:16:18,602 - INFO - Starting training for 10 epochs\n",
      "Training:   0%|                                          | 0/10 [00:00<?, ?it/s]2026-02-23 17:16:19,800 - INFO - \n",
      "Data loading time: 0.53s, Model training time: 0.67s\n",
      "Epoch [1/10], Loss: 0.98004657, LR: 0.00100000                                  \n",
      "Training:  10%|███▍                              | 1/10 [00:01<00:10,  1.20s/it]2026-02-23 17:16:20,676 - INFO - \n",
      "Data loading time: 0.48s, Model training time: 0.39s\n",
      "Epoch [2/10], Loss: 0.97727001, LR: 0.00100000                                  \n",
      "Training:  20%|██████▊                           | 2/10 [00:02<00:08,  1.01s/it]2026-02-23 17:16:21,567 - INFO - \n",
      "Data loading time: 0.49s, Model training time: 0.40s\n",
      "Epoch [3/10], Loss: 0.97846162, LR: 0.00100000                                  \n",
      "Training:  30%|██████████▏                       | 3/10 [00:02<00:06,  1.05it/s]2026-02-23 17:16:22,462 - INFO - \n",
      "Data loading time: 0.50s, Model training time: 0.39s\n",
      "Epoch [4/10], Loss: 0.97298443, LR: 0.00100000                                  \n",
      "Training:  40%|█████████████▌                    | 4/10 [00:03<00:05,  1.07it/s]2026-02-23 17:16:23,358 - INFO - \n",
      "Data loading time: 0.50s, Model training time: 0.39s\n",
      "Epoch [5/10], Loss: 0.97362638, LR: 0.00100000                                  \n",
      "Training:  50%|█████████████████                 | 5/10 [00:04<00:04,  1.09it/s]2026-02-23 17:16:24,234 - INFO - \n",
      "Data loading time: 0.48s, Model training time: 0.39s\n",
      "Epoch [6/10], Loss: 0.97462100, LR: 0.00100000                                  \n",
      "Training:  60%|████████████████████▍             | 6/10 [00:05<00:03,  1.11it/s]2026-02-23 17:16:25,129 - INFO - \n",
      "Data loading time: 0.50s, Model training time: 0.39s\n",
      "Epoch [7/10], Loss: 0.97134638, LR: 0.00100000                                  \n",
      "Training:  70%|███████████████████████▊          | 7/10 [00:06<00:02,  1.11it/s]2026-02-23 17:16:26,012 - INFO - \n",
      "Data loading time: 0.49s, Model training time: 0.39s\n",
      "Epoch [8/10], Loss: 0.96519387, LR: 0.00100000                                  \n",
      "Training:  80%|███████████████████████████▏      | 8/10 [00:07<00:01,  1.12it/s]2026-02-23 17:16:26,889 - INFO - \n",
      "Data loading time: 0.48s, Model training time: 0.39s\n",
      "Epoch [9/10], Loss: 0.96857136, LR: 0.00100000                                  \n",
      "Training:  90%|██████████████████████████████▌   | 9/10 [00:08<00:00,  1.12it/s]2026-02-23 17:16:27,772 - INFO - \n",
      "Data loading time: 0.49s, Model training time: 0.39s\n",
      "Epoch [10/10], Loss: 0.97418529, LR: 0.00100000                                 \n",
      "Training: 100%|█████████████████████████████████| 10/10 [00:09<00:00,  1.09it/s]\n",
      "2026-02-23 17:16:27,809 - INFO - Model saved to: ../saved_models/test_train_model\n"
     ]
    }
   ],
   "source": [
    "!python ./../train.py \\\n",
    "    --image_path \"./../data/img/\" \\\n",
    "    --label_path \"./../data/seg/\" \\\n",
    "    --prep_mode 4 \\\n",
    "    --epochs 10 \\\n",
    "    --learning_rate 1e-3 \\\n",
    "    --output_model ./../saved_models/test_train_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please be note that in a real case scenario, we would wish for a loss value below 0.4000 to generate reasonable segmentations. However, for the sake of speed, in this tutorial notebook we set the number of training epochs to 10 to demonstrate the usage. In addition, although we illustrate the usage of this module with a single image, we recommend that you train your models with a larger dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
